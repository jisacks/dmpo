# Experiment configuration
n_iters: 1000
n_epochs: 1
batch_size: 8
n_gpus: 1
log_folder: quadrotor_logs
exp_name: quadrotor_dmpo_zigzagyaw
dtype: float
env_device: cuda
seed: 0

# Rollout configuration
train_episode_len: 100
val_episode_len: 200
train_episodes: 1
val_episodes: 1
val_every: 10
break_if_done: False
use_condition: True
n_pretrain_epochs: 10
n_pretrain_steps: 1
n_val_envs: 32

# Training configuration
max_grad_norm: 1
num_workers: 0

# Dataset configuration
dataset_config:
  discount: 0.99
  gae_lambda: 0.95
  seq_length: 1
  stride: 1

# Environment configuration
env_name: quadrotor
dynamic_env: True
env_config:
  num_envs: 8
  config: ../config/envs/zigzagyaw.yaml
  action_is_mf: False
  use_delay_model: True
  delay_coeff: 0.4
  mass_range: [0.7, 1.3]
  delay_range: [0.2, 0.6]
  force_range: [-3.5, 3.5]
  force_is_z: True
  use_obs_noise: True
train_env_config:
  randomize_mass: True
  randomize_delay_coeff: True
  force_pert: True
val_env_config:
  randomize_mass: True
  randomize_delay_coeff: True
  force_pert: True

# Task configuration
task_config:
  task_config_file: ../config/mpc/quadrotor_zigzagyaw_mppi.yml

# Model configuration
model_config:
  model_type: dmpo_policy
  d_action: 4
  d_state: 13
  num_particles: 512
  horizon: 32
  gamma: 1.0
  top_k: 8
  init_mean: [0.3924, 0, 0 ,0]
  init_std: [0.1, 1., 1., 1.]
  mean_search_std: [0.1, 1., 1., 1.]
  std_search_std: [0.01, 0.1, 0.1, 0.1]
  learn_search_std: True
  learn_rollout_std: True
  is_delta: False
  is_gated: True
  is_residual: True
  d_cond: 56
  cond_mode: cat
  cond_actor: False
  cond_critic: True
  cond_shift: False
  critic_use_cost: False
  actor_use_state: False
  state_scale: null
  cond_scale: null
  mppi_params:
    temperature: 0.05
    step_size: 0.8
    scale_costs: True
  actor_params:
    net_type: mlp
    hidden_size: [256]
    act: relu
    init_scale: 0.001
  critic_params:
    net_type: mlp
    hidden_size: [1024]
    act: relu
    init_scale: 0.001
  shift_params:
    net_type: mlp
    hidden_size: [256]
    act: relu
    init_scale: 0.001

# Optimizer configuration
actor_optim_config:
  optim: Adam
  optim_args:
    lr: 0.000001

critic_optim_config:
  optim: Adam
  optim_args:
    lr: 0.0001

# Trainer configuration
trainer_config:
  clip_epsilon: 0.2
  std_clip_epsilon: 0.2
  entropy_penalty: 0.
  kl_penalty: 0.0
  model_subsets: [[actor, shift_model], [critic]]

